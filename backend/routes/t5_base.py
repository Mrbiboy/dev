import os

from flask import Flask, Blueprint, request, jsonify
from flask_jwt_extended import jwt_required
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import google.generativeai as genai
import torch
from dotenv import load_dotenv

# === Initialisation Flask ===
app = Flask(__name__)
t5_base_bp = Blueprint('t5', __name__)

# === Chargement du mod√®le T5 fine-tun√© ===
MODEL_PATH = "TahalliAnas/t5_base_ConfigFiles_fixer"
tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
t5_model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_PATH)

# === Configuration Gemini API ===
load_dotenv()
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")

genai.configure(api_key=GEMINI_API_KEY)
gemini_model = genai.GenerativeModel("gemini-1.5-flash")

@t5_base_bp.route("/t5", methods=["POST"])
@jwt_required()
def correct_dockerfile():
    data = request.get_json()
    dockerfile = data.get("dockerfile", "")

    if not dockerfile.strip():
        return jsonify({"error": "Le champ 'dockerfile' est requis"}), 400

    # --- √âtape 1 : G√©n√©ration du Dockerfile corrig√© avec T5 ---
    prompt = (
        "Fix security issues in this Dockerfile:\n"
        f"{dockerfile}"
    )

    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=512).to(t5_model.device)
    with torch.no_grad():
        outputs = t5_model.generate(**inputs, max_length=512)
        correction = tokenizer.decode(outputs[0], skip_special_tokens=True)

    # Nettoyage brut
    fixed_code = correction.strip()
    fixed_code = fixed_code.replace("\\n", "\n").replace("\\\\", "\\")

    # Ajout de retour √† la ligne entre les instructions Docker
    docker_instructions = [
        "FROM", "RUN", "CMD", "COPY", "ADD", "WORKDIR", "USER",
        "EXPOSE", "ENV", "ENTRYPOINT", "VOLUME", "LABEL", "ARG", "HEALTHCHECK"
    ]
    for instr in docker_instructions:
        fixed_code = fixed_code.replace(f" {instr} ", f"\n{instr} ")
        fixed_code = fixed_code.replace(f"{instr} ", f"\n{instr} ")

    fixed_code = fixed_code.strip()

    # --- √âtape 2 : G√©n√©ration de l'explication avec Gemini ---
    explanation_prompt = f"""
    Voici un fichier Docker avant et apr√®s correction :

    üîß Avant :
    {dockerfile}

    ‚úÖ Apr√®s :
    {fixed_code}

    Peux-tu expliquer les changements ligne par ligne pour montrer comment la version corrig√©e am√©liore la s√©curit√© ou les bonnes pratiques ?
    """

    try:
        gemini_response = gemini_model.generate_content(explanation_prompt)
        explanation = gemini_response.text.strip()
    except Exception as e:
        explanation = f"[Erreur Gemini] Impossible de g√©n√©rer l'explication : {e}"

    # --- R√©ponse finale ---
    return jsonify({
        "correction": fixed_code,
        "explanation": explanation
    }), 200